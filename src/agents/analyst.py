"""
åˆ†æå¸ˆæ™ºèƒ½ä½“ - è´Ÿè´£ç ”ç©¶æ•°æ®åˆ†æå’Œå†…å®¹ç­–ç•¥åˆ¶å®š
"""
import os
import sys
import logging
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, Any, List
import re
import json

# æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•å¹¶åŠ è½½ç¯å¢ƒå˜é‡
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent
env_path = project_root / '.env'

if env_path.exists():
    load_dotenv(env_path)

from crewai import Agent

class AnalystAgent:
    """åˆ†æå¸ˆæ™ºèƒ½ä½“ - ä¸“é—¨è´Ÿè´£ç ”ç©¶æ•°æ®åˆ†æå’Œå†…å®¹ç­–ç•¥åˆ¶å®š"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self._check_environment()
        self._initialize_tools()

    def _check_environment(self):
        """æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®"""
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("âŒ æœªæ‰¾åˆ° OPENAI_API_KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶é…ç½®")
        print("ğŸ”‘ OpenAI API Key: å·²è®¾ç½®")

    def _initialize_tools(self):
        """åˆå§‹åŒ–åˆ†æå·¥å…·"""
        try:
            # åˆ†æå¸ˆä¸»è¦ä½¿ç”¨å†…ç½®çš„LLMåˆ†æèƒ½åŠ›ï¼Œæš‚ä¸éœ€è¦å¤–éƒ¨å·¥å…·
            # å¯ä»¥åœ¨åç»­ç‰ˆæœ¬ä¸­æ·»åŠ æ›´å¤æ‚çš„åˆ†æå·¥å…·
            print("âœ… åˆ†æå¸ˆå·¥å…·åˆå§‹åŒ–æˆåŠŸï¼ˆä½¿ç”¨å†…ç½®åˆ†æèƒ½åŠ›ï¼‰")

        except Exception as e:
            self.logger.error(f"âŒ å·¥å…·åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            print(f"ğŸ’¡ è°ƒè¯•ä¿¡æ¯: {str(e)}")
            raise

    def create_agent(self, config: Dict[str, Any]) -> Agent:
        """
        åˆ›å»ºåˆ†æå¸ˆæ™ºèƒ½ä½“

        Args:
            config: æ™ºèƒ½ä½“é…ç½®å­—å…¸

        Returns:
            Agent: é…ç½®å¥½çš„åˆ†æå¸ˆæ™ºèƒ½ä½“
        """
        try:
            # åˆ†æå¸ˆæ™ºèƒ½ä½“ä¸»è¦ä½¿ç”¨LLMçš„å†…ç½®åˆ†æèƒ½åŠ›
            # ä¸éœ€è¦å¤–éƒ¨å·¥å…·ï¼Œä¸“æ³¨äºæ•°æ®åˆ†æå’Œç­–ç•¥åˆ¶å®š

            # åˆ›å»ºAgent
            agent = Agent(
                role=config['role'],
                goal=config['goal'],
                backstory=config['backstory'],
                tools=[],  # æš‚æ—¶ä¸ä½¿ç”¨å¤–éƒ¨å·¥å…·
                max_iter=config.get('max_iter', 2),
                max_execution_time=config.get('max_execution_time', 200),
                verbose=config.get('verbose', True),
                allow_delegation=config.get('allow_delegation', False),
                memory=True
            )

            self.logger.info("âœ… åˆ†æå¸ˆæ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
            print(f"ğŸ“Š åˆ†æå¸ˆæ™ºèƒ½ä½“å·²åˆ›å»º - {agent.role}")
            return agent

        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºåˆ†æå¸ˆæ™ºèƒ½ä½“å¤±è´¥: {str(e)}")
            raise

    def analyze_research_data(self, research_data: str) -> Dict[str, Any]:
        """
        åˆ†æç ”ç©¶æ•°æ®ï¼Œæå–å…³é”®æ´å¯Ÿ

        Args:
            research_data: ç ”ç©¶å‘˜æä¾›çš„ç ”ç©¶æ•°æ®

        Returns:
            Dict: åˆ†æç»“æœ
        """
        try:
            analysis_result = {
                'key_themes': self._extract_key_themes(research_data),
                'data_points': self._extract_data_points(research_data),
                'expert_opinions': self._extract_expert_opinions(research_data),
                'trends': self._identify_trends(research_data),
                'audience_insights': self._analyze_audience_relevance(research_data),
                'content_gaps': self._identify_content_gaps(research_data)
            }

            return analysis_result

        except Exception as e:
            self.logger.error(f"âŒ ç ”ç©¶æ•°æ®åˆ†æå¤±è´¥: {str(e)}")
            raise

    def _extract_key_themes(self, data: str) -> List[str]:
        """æå–å…³é”®ä¸»é¢˜"""
        # ç®€å•çš„å…³é”®è¯æå–é€»è¾‘
        # åœ¨å®é™…é¡¹ç›®ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæŠ€æœ¯
        themes = []

        # æŸ¥æ‰¾å¸¸è§çš„ä¸»é¢˜æ ‡è¯†è¯
        theme_patterns = [
            r'äººå·¥æ™ºèƒ½|AI|æœºå™¨å­¦ä¹ |æ·±åº¦å­¦ä¹ ',
            r'æ•°å­—åŒ–è½¬å‹|æ•°å­—åŒ–|æ™ºèƒ½åŒ–',
            r'è‡ªåŠ¨åŒ–|æ™ºèƒ½åˆ¶é€ |å·¥ä¸š4\.0',
            r'å¤§æ•°æ®|æ•°æ®åˆ†æ|æ•°æ®ç§‘å­¦',
            r'äº‘è®¡ç®—|è¾¹ç¼˜è®¡ç®—|åˆ†å¸ƒå¼',
            r'åŒºå—é“¾|åŠ å¯†è´§å¸|Web3',
        ]

        for pattern in theme_patterns:
            if re.search(pattern, data, re.IGNORECASE):
                themes.append(pattern.split('|')[0])

        return themes[:5]  # è¿”å›å‰5ä¸ªä¸»é¢˜

    def _extract_data_points(self, data: str) -> List[Dict[str, Any]]:
        """æå–æ•°æ®ç‚¹å’Œç»Ÿè®¡ä¿¡æ¯"""
        data_points = []

        # æŸ¥æ‰¾æ•°å­—å’Œç™¾åˆ†æ¯”
        number_patterns = [
            r'(\d+(?:\.\d+)?%)',  # ç™¾åˆ†æ¯”
            r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*(?:äº¿|ä¸‡|åƒ|ç™¾)',  # ä¸­æ–‡æ•°å­—å•ä½
            r'(\$\d+(?:,\d{3})*(?:\.\d+)?)\s*(?:billion|million|thousand|ä¸‡|äº¿)',  # è´§å¸
        ]

        for pattern in number_patterns:
            matches = re.findall(pattern, data, re.IGNORECASE)
            for match in matches[:3]:  # é™åˆ¶æ•°é‡
                data_points.append({
                    'value': match,
                    'context': self._get_context_around_match(data, match)
                })

        return data_points

    def _extract_expert_opinions(self, data: str) -> List[str]:
        """æå–ä¸“å®¶è§‚ç‚¹"""
        opinions = []

        # æŸ¥æ‰¾å¼•ç”¨å’Œè§‚ç‚¹æ ‡è¯†
        opinion_patterns = [
            r'ä¸“å®¶(?:è®¤ä¸º|è¡¨ç¤º|æŒ‡å‡º)[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]',
            r'ç ”ç©¶(?:æ˜¾ç¤º|è¡¨æ˜|å‘ç°)[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]',
            r'åˆ†æå¸ˆ(?:è®¤ä¸º|é¢„æµ‹|æŒ‡å‡º)[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]',
        ]

        for pattern in opinion_patterns:
            matches = re.findall(pattern, data)
            opinions.extend(matches[:2])  # æ¯ç§ç±»å‹æœ€å¤š2ä¸ª

        return opinions[:5]  # æ€»å…±æœ€å¤š5ä¸ªè§‚ç‚¹

    def _identify_trends(self, data: str) -> List[str]:
        """è¯†åˆ«è¶‹åŠ¿"""
        trends = []

        # æŸ¥æ‰¾è¶‹åŠ¿ç›¸å…³è¯æ±‡
        trend_patterns = [
            r'(?:ä¸Šå‡|å¢é•¿|æé«˜|å¢åŠ )[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]',
            r'(?:ä¸‹é™|å‡å°‘|é™ä½|è¡°å‡)[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]',
            r'(?:è¶‹åŠ¿|å‘å±•|å˜åŒ–|æ¼”è¿›)[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]',
        ]

        for pattern in trend_patterns:
            matches = re.findall(pattern, data)
            trends.extend(matches[:2])

        return trends[:5]

    def _analyze_audience_relevance(self, data: str) -> Dict[str, Any]:
        """åˆ†æç›®æ ‡å—ä¼—ç›¸å…³æ€§"""
        return {
            'target_groups': ['æŠ€æœ¯ä¸“ä¸šäººå£«', 'ä¼ä¸šå†³ç­–è€…', 'åˆ›ä¸šè€…'],
            'interest_level': 'high',
            'complexity_level': 'medium_to_high',
            'key_concerns': ['æŠ€æœ¯å‘å±•', 'å¸‚åœºæœºä¼š', 'æŠ•èµ„ä»·å€¼']
        }

    def _identify_content_gaps(self, data: str) -> List[str]:
        """è¯†åˆ«å†…å®¹ç©ºç™½ç‚¹"""
        gaps = [
            'ç¼ºå°‘å…·ä½“å®æ–½æ¡ˆä¾‹',
            'éœ€è¦æ›´å¤šæ•°æ®æ”¯æ’‘',
            'åº”è¯¥åŒ…å«é£é™©åˆ†æ',
            'å¯ä»¥å¢åŠ å¯¹æ¯”åˆ†æ'
        ]
        return gaps[:3]

    def _get_context_around_match(self, text: str, match: str, context_length: int = 50) -> str:
        """è·å–åŒ¹é…è¯å‘¨å›´çš„ä¸Šä¸‹æ–‡"""
        try:
            index = text.find(match)
            if index != -1:
                start = max(0, index - context_length)
                end = min(len(text), index + len(match) + context_length)
                return text[start:end].strip()
            return match
        except:
            return match

    def generate_content_outline(self, analysis_result: Dict[str, Any], content_type: str = "blog_post") -> Dict[str, Any]:
        """
        åŸºäºåˆ†æç»“æœç”Ÿæˆå†…å®¹å¤§çº²

        Args:
            analysis_result: åˆ†æç»“æœ
            content_type: å†…å®¹ç±»å‹

        Returns:
            Dict: å†…å®¹å¤§çº²
        """
        outline = {
            'title_suggestions': [
                f"æ·±åº¦è§£æï¼š{analysis_result['key_themes'][0] if analysis_result['key_themes'] else 'ä¸»é¢˜'}çš„æœ€æ–°å‘å±•",
                f"2025å¹´{analysis_result['key_themes'][0] if analysis_result['key_themes'] else 'æŠ€æœ¯'}è¶‹åŠ¿å…¨è§£è¯»",
                f"ä¸“å®¶è§‚ç‚¹ï¼š{analysis_result['key_themes'][0] if analysis_result['key_themes'] else 'è¡Œä¸š'}çš„æœºé‡ä¸æŒ‘æˆ˜"
            ],
            'structure': {
                'å¼•è¨€': 'ä»‹ç»ä¸»é¢˜èƒŒæ™¯å’Œé‡è¦æ€§',
                'ç°çŠ¶åˆ†æ': 'åŸºäºç ”ç©¶æ•°æ®çš„ç°çŠ¶è§£è¯»',
                'è¶‹åŠ¿æ´å¯Ÿ': 'æœªæ¥å‘å±•è¶‹åŠ¿é¢„æµ‹',
                'ä¸“å®¶è§‚ç‚¹': 'è¡Œä¸šä¸“å®¶çš„æ·±åº¦è§è§£',
                'å®é™…åº”ç”¨': 'å…·ä½“æ¡ˆä¾‹å’Œåº”ç”¨åœºæ™¯',
                'ç»“è®ºå»ºè®®': 'æ€»ç»“å’Œè¡ŒåŠ¨å»ºè®®'
            },
            'key_points': analysis_result.get('key_themes', []),
            'data_support': analysis_result.get('data_points', []),
            'seo_keywords': self._generate_seo_keywords(analysis_result),
            'target_audience': analysis_result.get('audience_insights', {}),
            'estimated_length': self._estimate_content_length(content_type)
        }

        return outline

    def _generate_seo_keywords(self, analysis_result: Dict[str, Any]) -> List[str]:
        """ç”ŸæˆSEOå…³é”®è¯"""
        keywords = []

        # ä»ä¸»é¢˜ä¸­æå–å…³é”®è¯
        themes = analysis_result.get('key_themes', [])
        keywords.extend(themes)

        # æ·»åŠ é€šç”¨SEOè¯æ±‡
        common_keywords = ['2025å¹´', 'æœ€æ–°è¶‹åŠ¿', 'æ·±åº¦åˆ†æ', 'ä¸“å®¶è§‚ç‚¹', 'å‘å±•å‰æ™¯']
        keywords.extend(common_keywords)

        return keywords[:10]  # è¿”å›å‰10ä¸ªå…³é”®è¯

    def _estimate_content_length(self, content_type: str) -> int:
        """ä¼°ç®—å†…å®¹é•¿åº¦"""
        length_map = {
            'blog_post': 1200,
            'article': 1500,
            'report': 2500,
            'news': 800,
            'tutorial': 2000
        }
        return length_map.get(content_type, 1200)


# æµ‹è¯•å‡½æ•°
def test_analyst_agent():
    """æµ‹è¯•åˆ†æå¸ˆæ™ºèƒ½ä½“"""
    print("ğŸ“Š æµ‹è¯•åˆ†æå¸ˆæ™ºèƒ½ä½“...")
    print(f"ğŸ“ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"ğŸ”‘ OPENAI_API_KEY: {'å·²è®¾ç½®' if os.getenv('OPENAI_API_KEY') else 'æœªè®¾ç½®'}")

    try:
        # æ¨¡æ‹Ÿé…ç½®
        config = {
            'role': 'å†…å®¹ç­–ç•¥åˆ†æå¸ˆ',
            'goal': 'åˆ†æç ”ç©¶æ•°æ®å¹¶åˆ¶å®šå†…å®¹ç­–ç•¥',
            'backstory': 'ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å†…å®¹ç­–ç•¥ä¸“å®¶',
            'max_iter': 2,
            'verbose': True
        }

        # åˆ›å»ºåˆ†æå¸ˆ
        print("ğŸš€ æ­£åœ¨åˆ›å»ºåˆ†æå¸ˆæ™ºèƒ½ä½“...")
        analyst = AnalystAgent()
        agent = analyst.create_agent(config)

        print("âœ… åˆ†æå¸ˆæ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
        print(f"ğŸ“‹ Agentè§’è‰²: {agent.role}")
        print(f"ğŸ¯ Agentç›®æ ‡: {agent.goal}")
        print(f"ğŸ› ï¸  å·¥å…·æ•°é‡: {len(agent.tools) if agent.tools else 0} (ä½¿ç”¨å†…ç½®åˆ†æèƒ½åŠ›)")

        # æµ‹è¯•åˆ†æåŠŸèƒ½
        sample_research = """
        äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨2024å¹´å–å¾—äº†é‡å¤§çªç ´ï¼Œå¸‚åœºè§„æ¨¡è¾¾åˆ°1840äº¿ç¾å…ƒï¼ŒåŒæ¯”å¢é•¿37%ã€‚
        ä¸“å®¶è®¤ä¸ºAIå°†åœ¨æœªæ¥5å¹´å†…å½»åº•æ”¹å˜åˆ¶é€ ä¸šã€‚ç ”ç©¶æ˜¾ç¤º85%çš„ä¼ä¸šæ­£åœ¨è€ƒè™‘AIæŠ•èµ„ã€‚
        """

        print("\nğŸ” æµ‹è¯•ç ”ç©¶æ•°æ®åˆ†æ...")
        analysis_result = analyst.analyze_research_data(sample_research)

        print("ğŸ“ˆ åˆ†æç»“æœ:")
        print(f"  - å…³é”®ä¸»é¢˜: {analysis_result['key_themes']}")
        print(f"  - æ•°æ®ç‚¹: {len(analysis_result['data_points'])} ä¸ª")
        print(f"  - ä¸“å®¶è§‚ç‚¹: {len(analysis_result['expert_opinions'])} ä¸ª")

        print("\nğŸ“ æµ‹è¯•å†…å®¹å¤§çº²ç”Ÿæˆ...")
        outline = analyst.generate_content_outline(analysis_result, "blog_post")
        print(f"  - æ ‡é¢˜å»ºè®®: {len(outline['title_suggestions'])} ä¸ª")
        print(f"  - å†…å®¹ç»“æ„: {len(outline['structure'])} ä¸ªéƒ¨åˆ†")
        print(f"  - SEOå…³é”®è¯: {outline['seo_keywords'][:5]}")

        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return True

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        print("\nğŸ”§ æ•…éšœæ’é™¤å»ºè®®:")
        print("1. æ£€æŸ¥ .env æ–‡ä»¶é…ç½®")
        print("2. ç¡®è®¤ OPENAI_API_KEY æœ‰æ•ˆ")
        print("3. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        return False


if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)

    # è¿è¡Œæµ‹è¯•
    test_analyst_agent()